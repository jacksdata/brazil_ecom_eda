{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code]\n\"\"\"\n    Script criado para consolidar classes e funções customizadas para o tratamento de textos e treinamento\n    de modelos de análise de sentimentos.\n\"\"\"\n\n\"\"\"\n--------------------------------------------\n---------- IMPORTANDO BIBLIOTECAS ----------\n--------------------------------------------\n\"\"\"\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import RSLPStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n\"\"\"\n--------------------------------------------\n---------- 1. FUNÇÕES PARA REGEX -----------\n--------------------------------------------\n\"\"\"\n\n# [RegEx] Padrão para encontrar quebra de linha e retorno de carro (\\n ou \\r)\ndef re_breakline(text_list, text_sub=' '):\n    \"\"\"\n    Args:\n    ----------\n    text_list: list object with text content to be prepared [type: list]\n    text_sub: string or pattern to substitute the regex pattern [type: string]\n    \"\"\"\n\n    return [re.sub('[\\n\\r]', text_sub, r) for r in text_list]\n\n\n# [RegEx] Padrão para encontrar sites ou hiperlinks\ndef re_hiperlinks(text_list, text_sub=' link '):\n    \"\"\"\n    Args:\n    ----------\n    text_list: list object with text content to be prepared [type: list]\n    text_sub: string or pattern to substitute the regex pattern [type: string]\n    \"\"\"\n\n    pattern = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n    return [re.sub(pattern, text_sub, r) for r in text_list]\n\n\n# [RegEx] Padrão para encontrar datas nos mais diversos formatos (dd/mm/yyyy, dd/mm/yy, dd.mm.yyyy, dd.mm.yy)\ndef re_dates(text_list, text_sub=' data '):\n    \"\"\"\n    Args:\n    ----------\n    text_list: list object with text content to be prepared [type: list]\n    text_sub: string or pattern to substitute the regex pattern [type: string]\n    \"\"\"\n\n    pattern = '([0-2][0-9]|(3)[0-1])(\\/|\\.)(((0)[0-9])|((1)[0-2]))(\\/|\\.)\\d{2,4}'\n    return [re.sub(pattern, text_sub, r) for r in text_list]\n\n\n# [RegEx] Padrão para encontrar valores financeiros (R$ ou  $)\ndef re_money(text_list, text_sub=' dinheiro '):\n    \"\"\"\n    Args:\n    ----------\n    text_list: list object with text content to be prepared [type: list]\n    text_sub: string or pattern to substitute the regex pattern [type: string]\n    \"\"\"\n\n    # Applying regex\n    pattern = '[R]{0,1}\\$[ ]{0,}\\d+(,|\\.)\\d+'\n    return [re.sub(pattern, text_sub, r) for r in text_list]\n\n\n# [RegEx] Padrão para encontrar números\ndef re_numbers(text_list, text_sub=' numero '):\n    \"\"\"\n    Args:\n    ----------\n    text_series: list object with text content to be prepared [type: list]\n    text_sub: string or pattern to substitute the regex pattern [type: string]\n    \"\"\"\n\n    # Applying regex\n    return [re.sub('[0-9]+', text_sub, r) for r in text_list]\n\n\n# [RegEx] Padrão para encontrar a palavra \"não\" em seus mais diversos formatos\ndef re_negation(text_list, text_sub=' negação '):\n    \"\"\"\n    Args:\n    ----------\n    text_series: list object with text content to be prepared [type: list]\n    text_sub: string or pattern to substitute the regex pattern [type: string]\n    \"\"\"\n\n    # Applying regex\n    return [re.sub('([nN][ãÃaA][oO]|[ñÑ]| [nN] )', text_sub, r) for r in text_list]\n\n\n# [RegEx] Padrão para limpar caracteres especiais\ndef re_special_chars(text_list, text_sub=' '):\n    \"\"\"\n    Args:\n    ----------\n    text_series: list object with text content to be prepared [type: list]\n    text_sub: string or pattern to substitute the regex pattern [type: string]\n    \"\"\"\n\n    # Applying regex\n    return [re.sub('\\W', text_sub, r) for r in text_list]\n\n\n# [RegEx] Padrão para limpar espaços adicionais\ndef re_whitespaces(text_list):\n    \"\"\"\n    Args:\n    ----------\n    text_series: list object with text content to be prepared [type: list]\n    \"\"\"\n\n    # Applying regex\n    white_spaces = [re.sub('\\s+', ' ', r) for r in text_list]\n    white_spaces_end = [re.sub('[ \\t]+$', '', r) for r in white_spaces]\n    return white_spaces_end\n\n\n\"\"\"\n--------------------------------------------\n------ 2. PROCESSAMENTO DE STOPWORDS -------\n--------------------------------------------\n\"\"\"\n\n# [StopWords] Função para remoção das stopwords e transformação de texto em minúsculas\ndef stopwords_removal(text, cached_stopwords=stopwords.words('portuguese')):\n    \"\"\"\n    Args:\n    ----------\n    text: list object where the stopwords will be removed [type: list]\n    cached_stopwords: stopwords to be applied on the process [type: list, default: stopwords.words('portuguese')]\n    \"\"\"\n\n    return [c.lower() for c in text.split() if c.lower() not in cached_stopwords]\n\n\n\"\"\"\n--------------------------------------------\n-------- 3. APLICAÇÃO DE STEMMING ----------\n--------------------------------------------\n\"\"\"\n\n# [Stemming] Função para aplicação de processo de stemming nas palavras\ndef stemming_process(text, stemmer=RSLPStemmer()):\n    \"\"\"\n    Args:\n    ----------\n    text: list object where the stopwords will be removed [type: list]\n    stemmer: type of stemmer to be applied [type: class, default: RSLPStemmer()]\n    \"\"\"\n\n    return [stemmer.stem(c) for c in text.split()]\n\n\n\"\"\"\n--------------------------------------------\n--- 4. EXTRAÇÃO DE FEATURES DE UM CORPUS ---\n--------------------------------------------\n\"\"\"\n\n# [Vocabulary] Função para aplicação de um vetorizador para criação de vocabulário\ndef extract_features_from_corpus(corpus, vectorizer, df=False):\n    \"\"\"\n    Args\n    ------------\n    text: text to be transformed into a document-term matrix [type: string]\n    vectorizer: engine to be used in the transformation [type: object]\n    \"\"\"\n\n    # Extracting features\n    corpus_features = vectorizer.fit_transform(corpus).toarray()\n    features_names = vectorizer.get_feature_names()\n\n    # Transforming into a dataframe to give interpetability to the process\n    df_corpus_features = None\n    if df:\n        df_corpus_features = pd.DataFrame(corpus_features, columns=features_names)\n\n    return corpus_features, df_corpus_features\n\n\n\"\"\"\n--------------------------------------------\n------ 5. DATAVIZ EM ANÁLISE DE TEXTO ------\n--------------------------------------------\n\"\"\"\n\n# [Viz] Função para retorno de DataFrame de contagem por ngram\ndef ngrams_count(corpus, ngram_range, n=-1, cached_stopwords=stopwords.words('portuguese')):\n    \"\"\"\n    Args\n    ----------\n    corpus: text to be analysed [type: pd.DataFrame]\n    ngram_range: type of n gram to be used on analysis [type: tuple]\n    n: top limit of ngrams to be shown [type: int, default: -1]\n    \"\"\"\n\n    # Using CountVectorizer to build a bag of words using the given corpus\n    vectorizer = CountVectorizer(stop_words=cached_stopwords, ngram_range=ngram_range).fit(corpus)\n    bag_of_words = vectorizer.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0)\n    words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n    total_list = words_freq[:n]\n\n    # Returning a DataFrame with the ngrams count\n    count_df = pd.DataFrame(total_list, columns=['ngram', 'count'])\n    return count_df\n\n\n\"\"\"\n--------------------------------------------\n-------- 6. PIPELINE DE DATA PREP ----------\n--------------------------------------------\n\"\"\"\n\n# [TEXT PREP] Classe para aplicar uma série de funções RegEx definidas em um dicionário\nclass ApplyRegex(BaseEstimator, TransformerMixin):\n\n    def __init__(self, regex_transformers):\n        self.regex_transformers = regex_transformers\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        # Applying all regex functions in the regex_transformers dictionary\n        for regex_name, regex_function in self.regex_transformers.items():\n            X = regex_function(X)\n\n        return X\n\n\n# [TEXT PREP] Classe para aplicar a remoção de stopwords em um corpus\nclass StopWordsRemoval(BaseEstimator, TransformerMixin):\n\n    def __init__(self, text_stopwords):\n        self.text_stopwords = text_stopwords\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return [' '.join(stopwords_removal(comment, self.text_stopwords)) for comment in X]\n\n\n# [TEXT PREP] Classe para aplicar o processo de stemming em um corpus\nclass StemmingProcess(BaseEstimator, TransformerMixin):\n\n    def __init__(self, stemmer):\n        self.stemmer = stemmer\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return [' '.join(stemming_process(comment, self.stemmer)) for comment in X]\n\n\n# [TEXT PREP] Classe para extração de features de um corpus (vocabulário / bag of words / TF-IDF)\nclass TextFeatureExtraction(BaseEstimator, TransformerMixin):\n\n    def __init__(self, vectorizer, train=True):\n        self.vectorizer = vectorizer\n        self.train = train\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        if self.train:\n            return self.vectorizer.fit_transform(X).toarray()\n        else:\n            return self.vectorizer.transform(X)\n\n\n\"\"\"\n--------------------------------------------\n--- 7. UTILITIES FOR SENTIMENT ANALYSIS ----\n--------------------------------------------\n\"\"\"\n\n# Defining a function to plot the sentiment of a given phrase\ndef sentiment_analysis(text, pipeline, vectorizer, model):\n    \"\"\"\n    Args\n    -----------\n    text: text string / phrase / review comment to be analysed [type: string]\n    pipeline: text prep pipeline built for preparing the corpus [type: sklearn.Pipeline]\n    model: classification model trained to recognize positive and negative sentiment [type: model]\n    \"\"\"\n\n    # Applying the pipeline\n    if type(text) is not list:\n        text = [text]\n    text_prep = pipeline.fit_transform(text)\n    matrix = vectorizer.transform(text_prep)\n\n    # Predicting sentiment\n    pred = model.predict(matrix)\n    proba = model.predict_proba(matrix)\n\n    # Plotting the sentiment and its score\n    fig, ax = plt.subplots(figsize=(5, 3))\n    if pred[0] == 1:\n        text = 'Positive'\n        class_proba = 100 * round(proba[0][1], 2)\n        color = 'seagreen'\n    else:\n        text = 'Negative'\n        class_proba = 100 * round(proba[0][0], 2)\n        color = 'crimson'\n    ax.text(0.5, 0.5, text, fontsize=50, ha='center', color=color)\n    ax.text(0.5, 0.20, str(class_proba) + '%', fontsize=14, ha='center')\n    ax.axis('off')\n    ax.set_title('Sentiment Analysis', fontsize=14)\n    plt.show()\n","metadata":{"_uuid":"803767bd-3973-4a37-92ca-c02bad4e2afd","_cell_guid":"8f584d18-1ca3-4f58-bb7c-7a0c04a9c032","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}