{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code]\n\"\"\"\nThis script is responsible for allocating custom function and classes to be used on Machine Learning and Data Science\nprojects. The code below can be used to make training and evaluating model process easier to developers so they\ncan focus on model improvement and code structure.\n\n\n--- SUMMARY ---\n\n1. Classification\n    1.1 Classifiers Analysis\n2. Clustering\n\n---------------------------------------------------------------\nWritten by Thiago Panini - Latest version: September 15th 2020\n---------------------------------------------------------------\n\"\"\"\n\n# Importing libraries\nimport pandas as pd\nimport numpy as np\nimport itertools\nimport time\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom viz_utils import format_spines, AnnotateBars\nfrom sklearn.model_selection import RandomizedSearchCV, cross_val_score, cross_val_predict, learning_curve\nfrom sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, roc_curve, confusion_matrix\nimport shap\n\nfrom sklearn.cluster import KMeans\n\n\n\"\"\"\n-----------------------------------\n-------- 1. CLASSIFICATION --------\n     1.1 Classifiers Analysis\n-----------------------------------\n\"\"\"\n\n\nclass BinaryClassifiersAnalysis:\n    \"\"\"\n    This class makes the work on training binary classifiers easier by bringing useful functions for training,\n    purposing search on hyperparameters space, evaluating metrics and much more\n    \"\"\"\n\n    def __init__(self):\n        self.classifiers_info = {}\n\n    def fit(self, classifiers, X, y, approach='', random_search=False, scoring='roc_auc', cv=5, verbose=5, n_jobs=-1):\n        \"\"\"\n        This function receives information from classifiers to be trained, the data to be used on training and other\n        parameters for fitting the model to the data.\n\n        Parameters\n        ----------\n        :param classifiers: dictionary containing estimators and hyperparameters inner dict [type: dict]\n        :param X: object containing features already prepared for training the model [type: np.array]\n        :param y: object containing the model target variable [type: np.array]\n        :param approach: string to be added on model's name as sufix for identifying purposes [type: string, default: '']\n        :param random_search: guides the application of Randomized Search on training [type: bool, default: True]\n        :param scoring: scoring metric to be optimized on random search [type: string, default: 'roc_auc']\n        :param cv: K-folds used on random search cross-validation [type: int, default: 5]\n        :param verbose: verbose param from RandomizedSearchCV [type: int, default: 5]\n        :param n_jobs: n_jobs param from RandomizedSarchCV [type: int, default: -1]\n\n        Return\n        ------\n        This method don't return anything but it fills some class attributes like self.classifiers_info dict\n\n        Application\n        -----------\n        # Creating dictionary object for storing models information\n        set_classifiers = {\n            'ModelName': {\n                'model': ClassifierEstimator(),\n                'params': clf_dict_params\n            }\n        }\n        trainer = BinaryClassifierAnalysis()\n        trainer.fit(set_classifiers, X_train_prep, y_train, random_search=True, cv=5)\n        \"\"\"\n\n        # Iterating trough every model in the dictionary of classifiers\n        for model_name, model_info in classifiers.items():\n            clf_key = model_name + approach\n            print(f'Training model {clf_key}\\n')\n\n            # Creating an empty dict for storing model information\n            self.classifiers_info[clf_key] = {}\n\n            # Application of RandomizedSearchCV\n            if random_search:\n                rnd_search = RandomizedSearchCV(model_info['model'], model_info['params'], scoring=scoring, cv=cv,\n                                                verbose=verbose, random_state=42, n_jobs=n_jobs)\n                rnd_search.fit(X, y)\n\n                # Saving the best estimator into the model's dict\n                self.classifiers_info[clf_key]['estimator'] = rnd_search.best_estimator_\n            else:\n                self.classifiers_info[clf_key]['estimator'] = model_info['model'].fit(X, y)\n\n    def compute_train_performance(self, model_name, estimator, X, y, cv=5):\n        \"\"\"\n        This function applies cross validation to retrieve useful metrics for the classification model.\n        In practice, this function would be called by another one (usually with compute_test_performance as well)\n\n        Parameters\n        ----------\n        :param model_name: key-string that identifies a model at self.classifiers_info dict [type: string]\n        :param estimator:\n        :param X: object containing features already prepared for training the model [type: np.array]\n        :param y: object containing the model target variable [type: np.array]\n        :param cv: k-folds for cross validation application on training evaluation\n\n        Return\n        ------\n        :return train_performance: DataFrame containing model metrics calculated using cross validation\n\n        Application\n        -----------\n        # Evaluating training performance using cross-validation\n        df_performances = trainer.compute_train_performance('DecisionTrees', trained_model, X_train, y_train, cv=5)\n        \"\"\"\n\n        # Computing metrics using cross validation\n        t0 = time.time()\n        accuracy = cross_val_score(estimator, X, y, cv=cv, scoring='accuracy').mean()\n        precision = cross_val_score(estimator, X, y, cv=cv, scoring='precision').mean()\n        recall = cross_val_score(estimator, X, y, cv=cv, scoring='recall').mean()\n        f1 = cross_val_score(estimator, X, y, cv=cv, scoring='f1').mean()\n\n        # Probas for calculating AUC\n        try:\n            y_scores = cross_val_predict(estimator, X, y, cv=cv, method='decision_function')\n        except:\n            # Tree based models don't have 'decision_function()' method, but 'predict_proba()'\n            y_probas = cross_val_predict(estimator, X, y, cv=cv, method='predict_proba')\n            y_scores = y_probas[:, 1]\n        auc = roc_auc_score(y, y_scores)\n\n        # Saving scores on self.classifiers_info dictionary\n        self.classifiers_info[model_name]['train_scores'] = y_scores\n\n        # Creating a DataFrame with metrics\n        t1 = time.time()\n        delta_time = t1 - t0\n        train_performance = {}\n        train_performance['model'] = model_name\n        train_performance['approach'] = f'Treino {cv} K-folds'\n        train_performance['acc'] = round(accuracy, 4)\n        train_performance['precision'] = round(precision, 4)\n        train_performance['recall'] = round(recall, 4)\n        train_performance['f1'] = round(f1, 4)\n        train_performance['auc'] = round(auc, 4)\n        train_performance['total_time'] = round(delta_time, 3)\n\n        return pd.DataFrame(train_performance, index=train_performance.keys()).reset_index(drop=True).loc[:0, :]\n\n    def compute_test_performance(self, model_name, estimator, X, y):\n        \"\"\"\n        This function retrieves metrics from the trained model on test data.\n        In practice, this function would be called by another one (usually with compute_train_performance as well)\n\n        Parameters\n        ----------\n        :param model_name: key-string that identifies a model at self.classifiers_info dict [type: string]\n        :param estimator:\n        :param X: object containing features already prepared for training the model [type: np.array]\n        :param y: object containing the model target variable [type: np.array]\n\n        Return\n        ------\n        :return test_performance: DataFrame containing model metrics calculated on test data\n\n        Application\n        -----------\n        # Evaluating test data performance\n        df_performances = trainer.compute_test_performance('DecisionTrees', trained_model, X_train, y_train)\n        \"\"\"\n\n        # Predicting data using the trained model and computing probabilities\n        t0 = time.time()\n        y_pred = estimator.predict(X)\n        y_proba = estimator.predict_proba(X)\n        y_scores = y_proba[:, 1]\n\n        # Retrieving metrics using test data\n        accuracy = accuracy_score(y, y_pred)\n        precision = precision_score(y, y_pred)\n        recall = recall_score(y, y_pred)\n        f1 = f1_score(y, y_pred)\n        auc = roc_auc_score(y, y_scores)\n\n        # Saving probabilities on treined classifiers dictionary\n        self.classifiers_info[model_name]['test_scores'] = y_scores\n\n        # Creating a DataFrame with metrics\n        t1 = time.time()\n        delta_time = t1 - t0\n        test_performance = {}\n        test_performance['model'] = model_name\n        test_performance['approach'] = f'Teste'\n        test_performance['acc'] = round(accuracy, 4)\n        test_performance['precision'] = round(precision, 4)\n        test_performance['recall'] = round(recall, 4)\n        test_performance['f1'] = round(f1, 4)\n        test_performance['auc'] = round(auc, 4)\n        test_performance['total_time'] = round(delta_time, 3)\n\n        return pd.DataFrame(test_performance, index=test_performance.keys()).reset_index(drop=True).loc[:0, :]\n\n    def evaluate_performance(self, X_train, y_train, X_test, y_test, cv=5, save=False, overwrite=True,\n                             performances_filepath='model_performances.csv'):\n        \"\"\"\n        This function centralizes the evaluating metric process by calling train and test evaluation functions.\n\n        Parameters\n        ----------\n        :param X_train: training data to be used on evaluation [np.array]\n        :param y_train: training target variable to be used on evaluation [type: np.array]\n        :param X_test: testing data to be used on evaluation [np.array]\n        :param y_test: testing target variable to be used on evaluation [type: np.array]\n        :param cv: K-folds used on cross validation step [type: int, default: 5]\n        :param save: flag that guides saving the final DataFrame with metrics [type: bool, default: False]\n        :param overwrite: flag that guides the overwriting of a saved metrics file [type: bool, default: True]\n        :param performances_filepath: path reference for saving model performances dataset [type: string,\n                                                                        default: 'model_performances.csv']\n\n        Return\n        ------\n        :return df_performance: DataFrame containing model metrics calculated on training and test data\n\n        Application\n        -----------\n        # Evaluating performance on training and testint\n        df_performance = trainer.evaluate_performance(X_train, y_train, X_test, y_test, save=True)\n        \"\"\"\n\n        # Iterating over each trained classifier at classifiers_info dictionary\n        df_performances = pd.DataFrame({})\n        for model_name, model_info in self.classifiers_info.items():\n\n            # Validating if the model was already trained (the key 'train_performance' will be at model_info dict if so)\n            if 'train_performance' in model_info.keys():\n                df_performances = df_performances.append(model_info['train_performance'])\n                df_performances = df_performances.append(model_info['test_performance'])\n                continue\n\n            # Returning the estimator for calling the evaluation functions\n            print(f'Evaluating model {model_name}\\n')\n            estimator = model_info['estimator']\n\n            # Retrieving training and testing metrics by calling inner functions\n            train_performance = self.compute_train_performance(model_name, estimator, X_train, y_train, cv=cv)\n            test_performance = self.compute_test_performance(model_name, estimator, X_test, y_test)\n\n            # Putting results on model's dictionary (classifiers_info)\n            self.classifiers_info[model_name]['train_performance'] = train_performance\n            self.classifiers_info[model_name]['test_performance'] = test_performance\n\n            # Building and unique DataFrame with performances retrieved\n            model_performance = train_performance.append(test_performance)\n            df_performances = df_performances.append(model_performance)\n\n            # Saving some attributes on model_info dictionary for further access\n            model_data = {\n                'X_train': X_train,\n                'y_train': y_train,\n                'X_test': X_test,\n                'y_test': y_test\n            }\n            model_info['model_data'] = model_data\n\n        # Saving the metrics file if applicable\n        if save:\n            # Adding information of measuring and execution time\n            cols_performance = list(df_performances.columns)\n            df_performances['anomesdia'] = datetime.now().strftime('%Y%m%d')\n            df_performances['anomesdia_datetime'] = datetime.now()\n            df_performances = df_performances.loc[:, ['anomesdia', 'anomesdia_datetime'] + cols_performance]\n\n            # Validating overwriting or append on data already saved\n            if overwrite:\n                df_performances.to_csv(performances_filepath, index=False)\n            else:\n                try:\n                    # If overwrite is False, tries reading existing metrics data and applying append on it\n                    log_performances = pd.read_csv(performances_filepath)\n                    full_performances = log_performances.append(df_performances)\n                    full_performances.to_csv(performances_filepath, index=False)\n                except FileNotFoundError:\n                    print('Log de performances do modelo não existente no caminho especificado. Salvando apenas o atual.')\n                    df_performances.to_csv(performances_filepath, index=False)\n\n        return df_performances\n\n    def feature_importance_analysis(self, features, specific_model=None, graph=True, ax=None, top_n=30,\n                                    palette='viridis', save=False, features_filepath='features_info.csv'):\n        \"\"\"\n        This function retrieves the feature importance from a given model. It can also build a bar chart\n        for top_n most important features and plot it on the notebook.\n\n        Paramters\n        ---------\n        :param features: list of model features used on training [type: list]\n        :param specific_model: information that guides the returning of feature importance for a specific model*\n        :param graph: flag that guides bar chart plotting at the end of execution [type: bool, default: True]\n        :param ax: axis for plotting the bar chart [type: matplotlib.axes, default: None]\n        :param top_n: parameter for showing up just top most important features [type: int, default: 30]\n        :param palette: color configuration for feature importance bar chart [type: string, default: 'viridis']\n        :param save: flag for saving the dataset returned [type: bool, default: False]\n        :param features_filepath: path for saving the feature iportance dataset [type: string, default: 'features_info.csv']\n\n        Returns\n        -------\n        :return: model_feature_importance: pandas DataFrame with feature importances extracted by trained models\n        \"\"\"\n\n        # Iterating over each trained classifiers on classifiers_info dictionary\n        feat_imp = pd.DataFrame({})\n        all_feat_imp = pd.DataFrame({})\n        for model_name, model_info in self.classifiers_info.items():\n            # Creating a pandas DataFrame with model feature importance\n            try:\n                importances = model_info['estimator'].feature_importances_\n            except:\n                # If the given model doesn't have the feature_importances_ method, just continue for the next\n                continue\n            # Preparing the dataset with useful information\n            feat_imp['feature'] = features\n            feat_imp['importance'] = importances\n            feat_imp['anomesdia'] = datetime.now().strftime('%Y%m')\n            feat_imp['anomesdia_datetime'] = datetime.now()\n            feat_imp.sort_values(by='importance', ascending=False, inplace=True)\n            feat_imp.reset_index(drop=True, inplace=True)\n\n            # Saving the feature iportance at model's dictionary (classifiers_info)\n            self.classifiers_info[model_name]['feature_importances'] = feat_imp\n            all_feat_imp = all_feat_imp.append(feat_imp)\n            all_feat_imp['model'] = model_name\n\n        # Retrieving feature importance for a specific model\n        if specific_model is not None:\n            try:\n                model_feature_importance = self.classifiers_info[specific_model]['feature_importances']\n                if graph:\n                    # Plotting the bar chart\n                    sns.barplot(x='importance', y='feature', data=model_feature_importance.iloc[:top_n, :],\n                                ax=ax, palette=palette)\n                    format_spines(ax, right_border=False)\n                    ax.set_title(f'Top {top_n} {specific_model} Features mais Relevantes', size=14, color='dimgrey')\n\n                # Saving features for a specific model\n                if save:\n                    model_feature_importance['model'] = specific_model\n                    order_cols = ['anomesdia', 'anomesdia_datetime', 'model', 'feature', 'importance']\n                    model_feature_importance = model_feature_importance.loc[:, order_cols]\n                    model_feature_importance.to_csv(features_filepath, index=False)\n                return model_feature_importance\n\n            except:\n                # Exception raised if the \"specific_model\" param doesn't match with any model's dictionary key\n                print(f'Classificador {specific_model} não existente nas chaves de classificadores treinados.')\n                print(f'Opções possíveis: {list(self.classifiers_info.keys())}')\n                return None\n\n        else:\n            # Validating feature importance saving if not passing specific_model param\n            if save:\n                order_cols = ['anomesdia', 'anomedia_datetime', 'model', 'feature', 'importance']\n                all_feat_imp = all_feat_imp.loc[:, order_cols]\n                all_feat_imp.to_csv(features_filepath, index=False)\n            return all_feat_imp\n\n        # Non-matching param combination (it can't be possible plotting bar chart for all models)\n        if graph and specific_model is None:\n            print('Por favor, escolha um modelo específico para visualizar o gráfico das feature importances')\n            return None\n\n    def plot_roc_curve(self, figsize=(16, 6)):\n        \"\"\"\n        This function iterates over each estimator in classifiers_info dictionary and plots the ROC Curve for\n        each one for training (first axis) and testing data (second axis)\n\n        Paramaters\n        ----------\n        :param figsize: figure size for the plot [type: tuple, default: (16, 6)]\n\n        Returns\n        -------\n        This function doesn't return anything but the matplotlib plot for ROC Curve\n\n        Application\n        -----------\n        trainer.plot_roc_curve()\n        \"\"\"\n\n        # Creating matplotlib figure and axis for ROC Curve plot\n        fig, axs = plt.subplots(ncols=2, figsize=figsize)\n\n        # Iterating over trained models\n        for model_name, model_info in self.classifiers_info.items():\n            # Returning y data for the model (training and testing)\n            y_train = model_info['model_data']['y_train']\n            y_test = model_info['model_data']['y_test']\n\n            # Returning scores already calculated after performance evaluation\n            train_scores = model_info['train_scores']\n            test_scores = model_info['test_scores']\n\n            # Calculating false positives and true positives rate\n            train_fpr, train_tpr, train_thresholds = roc_curve(y_train, train_scores)\n            test_fpr, test_tpr, test_thresholds = roc_curve(y_test, test_scores)\n\n            # Returning the auc metric for training and testing already calculated after model evaluation\n            train_auc = model_info['train_performance']['auc'].values[0]\n            test_auc = model_info['test_performance']['auc'].values[0]\n\n            # Plotting graph (training data)\n            plt.subplot(1, 2, 1)\n            plt.plot(train_fpr, train_tpr, linewidth=2, label=f'{model_name} auc={train_auc}')\n            plt.plot([0, 1], [0, 1], 'k--')\n            plt.axis([-0.02, 1.02, -0.02, 1.02])\n            plt.xlabel('False Positive Rate')\n            plt.ylabel('True Positive Rate')\n            plt.title(f'ROC Curve - Train Data')\n            plt.legend()\n\n            # Plotting graph (testing data)\n            plt.subplot(1, 2, 2)\n            plt.plot(test_fpr, test_tpr, linewidth=2, label=f'{model_name} auc={test_auc}')\n            plt.plot([0, 1], [0, 1], 'k--')\n            plt.axis([-0.02, 1.02, -0.02, 1.02])\n            plt.xlabel('False Positive Rate')\n            plt.ylabel('True Positive Rate')\n            plt.title(f'ROC Curve - Test Data', size=12)\n            plt.legend()\n\n        plt.show()\n\n    def custom_confusion_matrix(self, model_name, y_true, y_pred, classes, cmap, normalize=False):\n        \"\"\"\n        This function is used for plotting and customizing a confusion matrix for a specific model. In practice,\n        this function can be called by a top level one for plotting matrix for many models.\n\n        Parameters\n        ----------\n        :param model_name: key reference for extracting model's estimator from classifiers_dict [type: string]\n        :param y_true: label reference for the target variable [type: np.array]\n        :param y_pred: array of predictions given by the respective model [type: np.array]\n        :param classes: alias for classes [type: string]\n        :param cmap: this parameters guides the colorway for the matrix [type: matplotlib.colormap]\n        :param normalize: normalizes the entries for the matrix [type: bool, default: False]\n\n        Returns\n        -------\n        :return: This functions doesn't return any object besides of plotting the confusion matrix\n\n        Application\n        -----------\n        Please refer to the self.plot_confusion_matrix() function\n\n        \"\"\"\n\n        # Returning a confusion matrix given the labels and predictions passed as args\n        conf_mx = confusion_matrix(y_true, y_pred)\n\n        # Plotando matriz\n        plt.imshow(conf_mx, interpolation='nearest', cmap=cmap)\n        plt.colorbar()\n        tick_marks = np.arange(len(classes))\n\n        # Customizando eixos\n        plt.xticks(tick_marks, classes, rotation=45)\n        plt.yticks(tick_marks, classes)\n\n        # Customizando entradas\n        fmt = '.2f' if normalize else 'd'\n        thresh = conf_mx.max() / 2.\n        for i, j in itertools.product(range(conf_mx.shape[0]), range(conf_mx.shape[1])):\n            plt.text(j, i, format(conf_mx[i, j]),\n                     horizontalalignment='center',\n                     color='white' if conf_mx[i, j] > thresh else 'black')\n        plt.ylabel('True Label')\n        plt.xlabel('Predicted Label')\n        plt.title(f'{model_name}\\nConfusion Matrix', size=12)\n\n    def plot_confusion_matrix(self, classes, normalize=False, cmap=plt.cm.Blues):\n        \"\"\"\n        This function plots a confusion matrix for training and testing data for each classifier at\n        self.classifiers_dict dictionary\n\n        Parameters\n        ----------\n        :param classes: labels for the target variable [type: string]\n        :param normalize: flag that guides the normalization of matrix values [type: bool, default: False]\n        :param cmap: param that colorizes the matrix [type: plt.cm, default: plt.cm.Blues]\n\n        Returns\n        -------\n        This function doesn't return anything but the matplotlib plot for confusion matrix\n        \"\"\"\n\n        # Defining parameters for ploting\n        k = 1\n        nrows = len(self.classifiers_info.keys())\n        fig = plt.figure(figsize=(10, nrows * 4))\n        sns.set(style='white', palette='muted', color_codes=True)\n\n        # Iterating over each classifier\n        for model_name, model_info in self.classifiers_info.items():\n            # Returning data from each model\n            X_train = model_info['model_data']['X_train']\n            y_train = model_info['model_data']['y_train']\n            X_test = model_info['model_data']['X_test']\n            y_test = model_info['model_data']['y_test']\n\n            # Making predictions for training (cross validation) and testing for returning confusion matrix\n            train_pred = cross_val_predict(model_info['estimator'], X_train, y_train, cv=5)\n            test_pred = model_info['estimator'].predict(X_test)\n\n            # Plotting matrix (training data)\n            plt.subplot(nrows, 2, k)\n            self.custom_confusion_matrix(model_name + ' Train', y_train, train_pred, classes=classes, cmap=cmap,\n                                         normalize=normalize)\n            k += 1\n\n            # Plotting matrix (testing data)\n            plt.subplot(nrows, 2, k)\n            self.custom_confusion_matrix(model_name + ' Test', y_test, test_pred, classes=classes, cmap=plt.cm.Greens,\n                                         normalize=normalize)\n            k += 1\n\n        plt.tight_layout()\n        plt.show()\n\n    def plot_learning_curve(self, model_name, ax, ylim=None, cv=5, n_jobs=1, train_sizes=np.linspace(.1, 1.0, 10)):\n        \"\"\"\n        This function calculates and plots the learning curve for a trained model\n\n        Parameters\n        ----------\n        :param model_name: Key reference for extracting an estimator from classifiers_dict dictionary [type: string]\n        :param ax: axis reference for plotting the learning curve [type: matplotlib.axis]\n        :param ylim: configuration of the limit on plot vertical axis [type: int, default: None]\n        :param cv: k-folds used on cross validation [type: int, default: 5]\n        :param n_jobs: number of cores used on retrieving the learning curve params [type: int, default: 1]\n        :param train_sizes: array that guides the steps bins used on learning curve [type: np.array,\n                                                                                    default:np.linspace(.1, 1.0, 10)]\n\n        Returns\n        -------\n        This function doesn't return anything but the matplotlib plot for the learning curve\n\n        Application\n        -----------\n        # Plotting the learning curve for a specific model\n        fig, ax = plt.subplots(figsize=(16, 6))\n        trainer.plot_learning_curve(model_name='LightGBM', ax=ax)\n        \"\"\"\n\n        # Returning the model to be evaluated\n        try:\n            model = self.classifiers_info[model_name]\n        except:\n            print(f'Classificador {model_name} não foi treinado.')\n            print(f'Opções possíveis: {list(self.classifiers_info.keys())}')\n            return None\n\n        # Returning useful data for the model\n        X_train = model['model_data']['X_train']\n        y_train = model['model_data']['y_train']\n\n        # Calling the learning curve model for retrieving the scores for training and validation\n        train_sizes, train_scores, val_scores = learning_curve(model['estimator'], X_train, y_train, cv=cv,\n                                                               n_jobs=n_jobs, train_sizes=train_sizes)\n\n        # Computing averages and standard deviation (training and validation)\n        train_scores_mean = np.mean(train_scores, axis=1)\n        train_scores_std = np.std(train_scores, axis=1)\n        val_scores_mean = np.mean(val_scores, axis=1)\n        val_scores_std = np.std(val_scores, axis=1)\n\n        # Results on training data\n        ax.plot(train_sizes, train_scores_mean, 'o-', color='navy', label='Training Score')\n        ax.fill_between(train_sizes, (train_scores_mean - train_scores_std), (train_scores_mean + train_scores_std),\n                        alpha=0.1, color='blue')\n\n        # Results on cross validation\n        ax.plot(train_sizes, val_scores_mean, 'o-', color='red', label='Cross Val Score')\n        ax.fill_between(train_sizes, (val_scores_mean - val_scores_std), (val_scores_mean + val_scores_std),\n                        alpha=0.1, color='crimson')\n\n        # Customizing graph\n        ax.set_title(f'Model {model_name} - Learning Curve', size=14)\n        ax.set_xlabel('Training size (m)')\n        ax.set_ylabel('Score')\n        ax.grid(True)\n        ax.legend(loc='best')\n\n    def plot_score_distribution(self, model_name, shade=False):\n        \"\"\"\n        This function plots a kdeplot for training and testing data splitting by target class\n\n        Parameters\n        ----------\n        :param model_name: key reference for the trained model [type: string]\n        :param shade: shade param for seaborn's kdeplot [type: bool, default: False]\n\n        Returns\n        -------\n        This function doesn't return anything but the matplotlib plot for the score distribution\n\n        Application\n        -----------\n        # Ploting scores distribution for a model\n        plot_score_distribution(model_name='LightGBM', shade=True)\n        \"\"\"\n\n        # Returning the model to be evaluated\n        try:\n            model = self.classifiers_info[model_name]\n        except:\n            print(f'Classificador {model_name} não foi treinado.')\n            print(f'Opções possíveis: {list(self.classifiers_info.keys())}')\n            return None\n\n        # Retrieving y array for training and testing data\n        y_train = model['model_data']['y_train']\n        y_test = model['model_data']['y_test']\n\n        # Retrieving training and testing scores\n        train_scores = model['train_scores']\n        test_scores = model['test_scores']\n\n        # Plotting scores distribution\n        fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(16, 5))\n        sns.kdeplot(train_scores[y_train == 1], ax=axs[0], label='y=1', shade=shade, color='darkslateblue')\n        sns.kdeplot(train_scores[y_train == 0], ax=axs[0], label='y=0', shade=shade, color='crimson')\n        sns.kdeplot(test_scores[y_test == 1], ax=axs[1], label='y=1', shade=shade, color='darkslateblue')\n        sns.kdeplot(test_scores[y_test == 0], ax=axs[1], label='y=0', shade=shade, color='crimson')\n\n        # Customizing plots\n        format_spines(axs[0], right_border=False)\n        format_spines(axs[1], right_border=False)\n        axs[0].set_title('Score Distribution - Training Data', size=12, color='dimgrey')\n        axs[1].set_title('Score Distribution - Testing Data', size=12, color='dimgrey')\n        plt.suptitle(f'Score Distribution: a Probability Approach for {model_name}\\n', size=14, color='black')\n        plt.show()\n\n    def plot_score_bins(self, model_name, bin_range):\n        \"\"\"\n        This function plots a score distribution based on quantity of each class in a specific bin_range set\n\n        Parameters\n        ----------\n        :param model_name: key reference for the trained model [type: string]\n        :param bin_range: defines a range of splitting the bins array [type: float]\n\n        Returns\n        -------\n        This function doesn't return anything but the matplotlib plot for the score bins distribution\n\n        Application\n        -----------\n        # Ploting scores distribution for a model in another approach\n        plot_score_bins(model_name='LightGBM', bin_range=0.1)\n        \"\"\"\n\n        # Returning the model to be evaluated\n        try:\n            model = self.classifiers_info[model_name]\n        except:\n            print(f'Classificador {model_name} não foi treinado.')\n            print(f'Opções possíveis: {list(self.classifiers_info.keys())}')\n            return None\n\n        # Creating the bins array\n        bins = np.arange(0, 1.01, bin_range)\n        bins_labels = [str(round(list(bins)[i - 1], 2)) + ' a ' + str(round(list(bins)[i], 2)) for i in range(len(bins))\n                       if i > 0]\n\n        # Retrieving the train scores and creating a DataFrame\n        train_scores = model['train_scores']\n        y_train = model['model_data']['y_train']\n        df_train_scores = pd.DataFrame({})\n        df_train_scores['scores'] = train_scores\n        df_train_scores['target'] = y_train\n        df_train_scores['faixa'] = pd.cut(train_scores, bins, labels=bins_labels)\n\n        # Computing the distribution for each bin\n        df_train_rate = pd.crosstab(df_train_scores['faixa'], df_train_scores['target'])\n        df_train_percent = df_train_rate.div(df_train_rate.sum(1).astype(float), axis=0)\n\n        # Retrieving the test scores and creating a DataFrame\n        test_scores = model['test_scores']\n        y_test = model['y_test']\n        df_test_scores = pd.DataFrame({})\n        df_test_scores['scores'] = test_scores\n        df_test_scores['target'] = y_test\n        df_test_scores['faixa'] = pd.cut(test_scores, bins, labels=bins_labels)\n\n        # Computing the distribution for each bin\n        df_test_rate = pd.crosstab(df_test_scores['faixa'], df_test_scores['target'])\n        df_test_percent = df_test_rate.div(df_test_rate.sum(1).astype(float), axis=0)\n\n        # Defining figure for plotting\n        fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(16, 12))\n\n        # Plotting the bar chart for each bin\n        for df_scores, ax in zip([df_train_scores, df_test_scores], [axs[0, 0], axs[0, 1]]):\n            sns.countplot(x='faixa', data=df_scores, hue='target', ax=ax, palette=['darkslateblue', 'crimson'])\n            AnnotateBars(n_dec=0, color='dimgrey').vertical(ax)\n            ax.legend(loc='upper right')\n            format_spines(ax, right_border=False)\n\n        # Plotting percentage for each class in each bin\n        for df_percent, ax in zip([df_train_percent, df_test_percent], [axs[1, 0], axs[1, 1]]):\n            df_percent.plot(kind='bar', ax=ax, stacked=True, color=['darkslateblue', 'crimson'], width=0.6)\n\n            # Customizing plots\n            for p in ax.patches:\n                # Collecting params for labeling\n                height = p.get_height()\n                width = p.get_width()\n                x = p.get_x()\n                y = p.get_y()\n\n                # Formatting params and putting into the graph\n                label_text = f'{round(100 * height, 1)}%'\n                label_x = x + width - 0.30\n                label_y = y + height / 2\n                ax.text(label_x, label_y, label_text, ha='center', va='center', color='white',\n                        fontweight='bold', size=10)\n            format_spines(ax, right_border=False)\n\n        # Final definitions\n        axs[0, 0].set_title('Quantity of each Class by Range - Train', size=12, color='dimgrey')\n        axs[0, 1].set_title('Quantity of each Class by Range - Test', size=12, color='dimgrey')\n        axs[1, 0].set_title('Percentage of each Class by Range - Train', size=12, color='dimgrey')\n        axs[1, 1].set_title('Percentage of each Class by Range - Test', size=12, color='dimgrey')\n        plt.suptitle(f'Score Distribution by Range - {model_name}\\n', size=14, color='black')\n        plt.tight_layout()\n        plt.show()\n\n    def shap_analysis(self, model_name, features):\n        \"\"\"\n        This function brings a shap analysis for each feature into the model\n\n        Parameters\n        ----------\n        :param model_name: key reference for the trained model [type: string]\n        :param features: features list for the model [type: list]\n\n        Returns\n        -------\n        This function doesn't return anything but the shap plot analysis\n\n        Application\n        -----------\n        # Executing a shap analysis\n        trainer.shap_analysis(model_name='LightGBM')\n        \"\"\"\n\n        # Returning the model to be evaluated\n        try:\n            model = self.classifiers_info[model_name]\n        except:\n            print(f'Classificador {model_name} não foi treinado.')\n            print(f'Opções possíveis: {list(self.classifiers_info.keys())}')\n            return None\n\n        # Retrieving training data\n        X_train = model['X_train']\n\n        # Applying shap approach\n        explainer = shap.TreeExplainer(model)\n        df_train = pd.DataFrame(X_train, columns=features)\n        shap_values = explainer.shap_values(df_train)\n\n        # Plotting a summary plot using shap\n        shap.summary_plot(shap_values[1], df_train)\n\n\ndef cross_val_performance(estimator, X, y, cv=5):\n    \"\"\"\n    This function applies cross validation to retrieve useful metrics for the classification model.\n    In practice, this function would be called by another one (usually with compute_test_performance as well)\n\n    Parameters\n    ----------\n    :param estimator: a trained model to be used on evaluation [type: model]\n    :param X: object containing features already prepared for training the model [type: np.array]\n    :param y: object containing the model target variable [type: np.array]\n    :param cv: k-folds for cross validation application on training evaluation\n\n    Return\n    ------\n    :return train_performance: DataFrame containing model metrics calculated using cross validation\n\n    Application\n    -----------\n    # Evaluating training performance using cross-validation\n    df_performances = trainer.compute_train_performance('DecisionTrees', trained_model, X_train, y_train, cv=5)\n    \"\"\"\n\n    # Computing metrics using cross validation\n    t0 = time.time()\n    accuracy = cross_val_score(estimator, X, y, cv=cv, scoring='accuracy').mean()\n    precision = cross_val_score(estimator, X, y, cv=cv, scoring='precision').mean()\n    recall = cross_val_score(estimator, X, y, cv=cv, scoring='recall').mean()\n    f1 = cross_val_score(estimator, X, y, cv=cv, scoring='f1').mean()\n\n    # Probas for calculating AUC\n    try:\n        y_scores = cross_val_predict(estimator, X, y, cv=cv, method='decision_function')\n    except:\n        # Tree based models don't have 'decision_function()' method, but 'predict_proba()'\n        y_probas = cross_val_predict(estimator, X, y, cv=cv, method='predict_proba')\n        y_scores = y_probas[:, 1]\n    auc = roc_auc_score(y, y_scores)\n\n    # Creating a DataFrame with metrics\n    t1 = time.time()\n    delta_time = t1 - t0\n    train_performance = {}\n    train_performance['model'] = estimator.__class__.__name__\n    train_performance['approach'] = 'Final Model'\n    train_performance['acc'] = round(accuracy, 4)\n    train_performance['precision'] = round(precision, 4)\n    train_performance['recall'] = round(recall, 4)\n    train_performance['f1'] = round(f1, 4)\n    train_performance['auc'] = round(auc, 4)\n    train_performance['total_time'] = round(delta_time, 3)\n    df_performance = pd.DataFrame(train_performance, index=train_performance.keys()).reset_index(drop=True).loc[:0, :]\n\n    # Adding information of measuring and execution time\n    cols_performance = list(df_performance.columns)\n    df_performance['anomesdia'] = datetime.now().strftime('%Y%m%d')\n    df_performance['anomesdia_datetime'] = datetime.now()\n    df_performance = df_performance.loc[:, ['anomesdia', 'anomesdia_datetime'] + cols_performance]\n\n    return df_performance\n    \n\n\"\"\"\n-----------------------------------\n---------- 2. CLUSTERING ----------\n-----------------------------------\n\"\"\"\n\n\ndef elbow_method_kmeans(df, K_min, K_max, figsize=(16, 6)):\n    \"\"\"\n    This function applies the elbow method using K-means clustering algorithm in order to select the best number of\n    k clusters for the task\n\n    Parameters\n    ----------\n    :param df: DataFrame with data to be clustered [type: pd.DataFrame]\n    :param K_min: minimum index of K to be evaluated on elbow method [type: int]\n    :param K_max: maximum index of K to be evaluated on elbow method [type: int]\n    :param figsize: figure dimension for the matplotlib plot [type: tuple, default: (16, 5)]\n\n    Returns\n    -------\n    :return: This function doesn't return anything but the elbow method plot for further analysis\n\n    Application\n    -----------\n    # Looking at the elbow graphic for selection the best k number of clusters\n    elbow_method_kmeans(df=data, K_min=1, K_max=8)\n    \"\"\"\n\n    # Training differente K-Means models for each number of flusters in range K_min, K_max\n    square_dist = []\n    for k in range(K_min, K_max+1):\n        km = KMeans(n_clusters=k)\n        km.fit(df)\n        # Computing and keeping the square distances for each model configuration\n        square_dist.append(km.inertia_)\n\n    # Plotting elbow analysis\n    fig, ax = plt.subplots(figsize=figsize)\n    sns.lineplot(x=range(K_min, K_max), y=square_dist, color='cornflowerblue', marker='o')\n\n    # Customizing the graph\n    format_spines(ax, right_border=False)\n    ax.set_title('Elbow Method - KMeans Model', size=14, color='dimgrey')\n    ax.set_xlabel('Total of Clusters (K)')\n    ax.set_ylabel('Euclidian Distance')\n    plt.show()\n\n\ndef plot_kmeans_clusters_2d(df, k_means, figsize=(16, 6), cmap='viridis'):\n    \"\"\"\n    This function plots and customizes a 2D result from a trained K-means algorithm\n\n    Parameters\n    ----------\n    :param df: DataFrame with the two columns used on training the K-means algorithm [type: pd.DataFrame]\n    :param kmeans: trained K-means model [type: model]\n    :param figsize: figure dimension for the plot [type: tuple, default: (16, 6)]\n    :param cmap: param for coloring the plot with a specific palette [type: string, default: 'viridis]\n\n    Returns\n    -------\n    :return: This function doesn't return anything but the K-means algorithm plot\n\n    Application\n    -----------\n    # Visualizing the kmeans result on a trained 2D model data\n    model = KMeans(n_clusters=3)\n    cluster_data = df.loc[:, two_columns_list]\n    k_means.fit(cluster_data)\n    plot_kmeans_clusters_2d(cluster_data, model)\n    \"\"\"\n\n    # Computing y_kmeans and centers\n    y_kmeans = k_means.predict(df)\n    centers = k_means.cluster_centers_\n\n    # REtrieving values and defining layout\n    variaveis = df.columns\n    X = df.values\n    sns.set(style='white', palette='muted', color_codes=True)\n\n    # Plotting the scatterplot chart\n    fig, ax = plt.subplots(figsize=figsize)\n    ax.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap=cmap)\n    ax.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)\n\n    # Customizing the graph\n    ax.set_title(f'K_Means aplicado entre {variaveis[0].upper()} e {variaveis[1].upper()}', size=14, color='dimgrey')\n    format_spines(ax, right_border=False)\n    ax.set_xlabel(variaveis[0])\n    ax.set_ylabel(variaveis[1])\n    plt.show()","metadata":{"_uuid":"7fbe38f2-f54d-43f6-a3ba-9bae5d1f6437","_cell_guid":"5d4b78e1-2da4-4cc1-9486-572fe1dde66c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}